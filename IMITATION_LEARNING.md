# Imitation Learning for MicroDuck

This document explains how to use imitation learning with reference motions in the MicroDuck environment.

## Overview

The imitation learning system allows the robot to learn from reference motions generated by a motion planner (Placo walk engine). The system:

1. Loads polynomial-fitted reference motions from a pickle file
2. Tracks the gait phase during training
3. Rewards the policy for matching the reference motion
4. Provides phase observations to help the policy coordinate movements

The imitation reward is **only active during training**, not during inference/play mode. However, the phase observation is available in both modes.

## Setup

### 1. Generate Reference Motions

First, generate reference motion JSON files using the `Open_Duck_reference_motion_generator`:

```bash
cd ~/MISC/Open_Duck_reference_motion_generator
python scripts/auto_waddle.py --duck microduck --sweep --output_dir ./out
```

This will create JSON files with names like `0_-0.03_-0.02_-0.5.json` where the numbers represent `id_dx_dy_dtheta` (forward velocity, lateral velocity, angular velocity).

### 2. Generate Polynomial Coefficients

Convert the JSON files to polynomial coefficients using the existing script:

```bash
cd ~/MISC/Open_Duck_reference_motion_generator
python scripts/fit_poly.py --ref_motion ./out
```

This creates `polynomial_coefficients.pkl` containing 15th-degree polynomial coefficients fitted to each reference motion. Copy or symlink this file to your mjlab_microduck data directory:

```bash
cp polynomial_coefficients.pkl ~/MISC/mjlab_microduck/src/mjlab_microduck/data/reference_motions.pkl
# OR symlink it
ln -s ~/MISC/Open_Duck_reference_motion_generator/polynomial_coefficients.pkl \
      ~/MISC/mjlab_microduck/src/mjlab_microduck/data/reference_motions.pkl
```

## Usage

### Training with Imitation Learning

Once you have the reference motion file in place, simply use the imitation learning task:

```bash
uv run train Mjlab-Velocity-Flat-MicroDuck-Imitation --env.scene.num-envs 2048
```

This will train with the imitation reward enabled. The task automatically loads the reference motion file from:
- Default location: `~/MISC/mjlab_microduck/src/mjlab_microduck/data/reference_motions.pkl`
- Or set via environment variable: `export MICRODUCK_REFERENCE_MOTION_PATH=/path/to/your/file.pkl`

### Inference/Play Mode

Play mode works the same way - the imitation reward is disabled but the phase observation remains:

```bash
uv run play Mjlab-Velocity-Flat-MicroDuck-Imitation --wandb-run-path <...>
```

### Alternative: Environment Variable

You can also set a custom path:

```bash
export MICRODUCK_REFERENCE_MOTION_PATH=~/MISC/Open_Duck_reference_motion_generator/polynomial_coefficients.pkl
uv run train Mjlab-Velocity-Flat-MicroDuck-Imitation --env.scene.num-envs 2048
```

## Architecture

### Reference Motion Format

Each reference motion is keyed by velocity: `"dx_dy_dtheta"` (e.g., `"0.01_0.0_-0.4"`).

The motion contains:
- **joints_pos** (14 DOF): Joint positions (excluding neck/head)
- **joints_vel** (14 DOF): Joint velocities
- **foot_contacts** (2): Binary contact states for left/right feet
- **base_linear_vel** (3): Base linear velocity in world frame
- **base_angular_vel** (3): Base angular velocity in world frame

### Imitation Reward Components

The imitation reward is a weighted sum of:

- **Joint position tracking** (weight=15.0): Negative squared error
- **Joint velocity tracking** (weight=0.001): Negative squared error
- **Linear velocity XY** (weight=1.0): Exponential reward
- **Linear velocity Z** (weight=1.0): Exponential reward
- **Angular velocity XY** (weight=0.5): Exponential reward
- **Angular velocity Z** (weight=0.5): Exponential reward
- **Foot contacts** (weight=1.0): Binary matching reward

The reward is only computed when the commanded velocity magnitude exceeds a threshold (default: 0.01).

### Phase Observation

The policy receives a 2D phase observation: `[cos(phase * 2π), sin(phase * 2π)]`

This provides a continuous, periodic representation of where the robot is in the gait cycle, helping it learn coordinated movements.

### Motion Matching

During execution:
1. The system finds the reference motion closest to the current commanded velocity
2. The phase is tracked and updated each timestep based on the motion's period
3. The polynomial is evaluated at the current phase to get the reference state
4. The reward compares the robot's actual state to the reference state

## Files

- `src/mjlab_microduck/reference_motion.py`: Reference motion loader
- `src/mjlab_microduck/tasks/mdp.py`: Imitation reward and phase observation functions
- `src/mjlab_microduck/tasks/microduck_velocity_env_cfg.py`: Environment configuration
- `src/mjlab_microduck/data/`: Directory for storing reference motion files

**Note**: Use `~/MISC/Open_Duck_reference_motion_generator/scripts/fit_poly.py` to generate polynomial coefficients from reference motions.

## Tuning

You can adjust the reward weights in the environment configuration:

```python
cfg.rewards["imitation"].params["weight_joint_pos"] = 15.0
cfg.rewards["imitation"].params["weight_joint_vel"] = 1e-3
cfg.rewards["imitation"].params["weight_lin_vel_xy"] = 1.0
cfg.rewards["imitation"].params["weight_lin_vel_z"] = 1.0
cfg.rewards["imitation"].params["weight_ang_vel_xy"] = 0.5
cfg.rewards["imitation"].params["weight_ang_vel_z"] = 0.5
cfg.rewards["imitation"].params["weight_contact"] = 1.0
```

You can also adjust the overall imitation reward weight:

```python
cfg.rewards["imitation"].weight = 1.0  # Adjust this value
```
