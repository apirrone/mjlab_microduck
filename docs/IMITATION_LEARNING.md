# Imitation Learning for MicroDuck

This document explains how to use imitation learning with reference motions in the MicroDuck environment.

## Overview

The imitation learning system allows the robot to learn from reference motions generated by a motion planner (Placo walk engine). The system:

1. Loads polynomial-fitted reference motions from a pickle file
2. Tracks the gait phase during training
3. Rewards the policy for matching the reference motion
4. Provides phase observations to help the policy coordinate movements

The imitation reward is **only active during training**, not during inference/play mode. However, the phase observation is available in both modes.

## Setup

### 1. Generate Reference Motions

First, generate reference motion JSON files using the `Open_Duck_reference_motion_generator`:

```bash
cd ~/MISC/Open_Duck_reference_motion_generator
python scripts/auto_waddle.py --duck microduck --sweep --output_dir ./out
```

This will create JSON files with names like `0_-0.03_-0.02_-0.5.json` where the numbers represent `id_dx_dy_dtheta` (forward velocity, lateral velocity, angular velocity).

### 2. Generate Polynomial Coefficients

Convert the JSON files to polynomial coefficients using the existing script:

```bash
cd ~/MISC/Open_Duck_reference_motion_generator
python scripts/fit_poly.py --ref_motion ./out
```

This creates `polynomial_coefficients.pkl` containing 15th-degree polynomial coefficients fitted to each reference motion. Copy or symlink this file to your mjlab_microduck data directory:

```bash
cp polynomial_coefficients.pkl ~/MISC/mjlab_microduck/src/mjlab_microduck/data/reference_motions.pkl
# OR symlink it
ln -s ~/MISC/Open_Duck_reference_motion_generator/polynomial_coefficients.pkl \
      ~/MISC/mjlab_microduck/src/mjlab_microduck/data/reference_motions.pkl
```

## Usage

### Training with Imitation Learning

Once you have the reference motion file in place, simply use the imitation learning task:

```bash
uv run train Mjlab-Velocity-Flat-MicroDuck-Imitation --env.scene.num-envs 2048
```

This will train with the imitation reward enabled. The task automatically loads the reference motion file from:
- Default location: `~/MISC/mjlab_microduck/src/mjlab_microduck/data/reference_motions.pkl`
- Or set via environment variable: `export MICRODUCK_REFERENCE_MOTION_PATH=/path/to/your/file.pkl`

### Inference/Play Mode

Play mode works the same way - the imitation reward is disabled but the phase observation remains:

```bash
uv run play Mjlab-Velocity-Flat-MicroDuck-Imitation --wandb-run-path <...>
```

### Alternative: Environment Variable

You can also set a custom path:

```bash
export MICRODUCK_REFERENCE_MOTION_PATH=~/MISC/Open_Duck_reference_motion_generator/polynomial_coefficients.pkl
uv run train Mjlab-Velocity-Flat-MicroDuck-Imitation --env.scene.num-envs 2048
```

## Architecture

### Reference Motion Format

Each reference motion is keyed by velocity: `"dx_dy_dtheta"` (e.g., `"0.01_0.0_-0.4"`).

The motion contains:
- **joints_pos** (14 DOF): Joint positions (all joints including legs and head)
  - Left leg (5): hip_yaw, hip_roll, hip_pitch, knee, ankle
  - Head (4): neck_pitch, head_pitch, head_yaw, head_roll
  - Right leg (5): hip_yaw, hip_roll, hip_pitch, knee, ankle
- **joints_vel** (14 DOF): Joint velocities
- **foot_contacts** (2): Binary contact states for left/right feet
- **base_linear_vel** (3): Base linear velocity in world frame
- **base_angular_vel** (3): Base angular velocity in world frame

### Imitation Reward Components

The imitation reward is a weighted sum of:

- **Joint position tracking** (weight=15.0): Negative squared error
- **Joint velocity tracking** (weight=0.001): Negative squared error
- **Linear velocity XY** (weight=1.0): Exponential reward
- **Linear velocity Z** (weight=1.0): Exponential reward
- **Angular velocity XY** (weight=0.5): Exponential reward
- **Angular velocity Z** (weight=0.5): Exponential reward
- **Foot contacts** (weight=1.0): Binary matching reward

The reward is only non-zero when the commanded velocity magnitude exceeds a threshold (default: 0.01). This prevents the policy from moving when there is no command.

### Phase Observation

The policy (actor) receives a 2D phase observation: `[cos(phase * 2π), sin(phase * 2π)]`

This provides a continuous, periodic representation of where the robot is in the gait cycle, helping it learn coordinated movements.

### Privileged Observations (Critic Only)

During training, the critic receives additional privileged information that is not available to the actor or at deployment:

- **Full reference motion state** (36D):
  - Reference joint positions (14)
  - Reference joint velocities (14)
  - Reference foot contacts (2)
  - Reference base linear velocity (3)
  - Reference base angular velocity (3)

This asymmetric actor-critic setup allows the critic to better evaluate states by knowing the complete reference trajectory, while the actor must learn to track the motion using only the phase signal. This is a common technique in imitation learning that improves learning efficiency.

### Observation Dimensions

**Actor (Policy) Observations**: 53D
- base_ang_vel (3)
- projected_gravity (3)
- joint_pos (14)
- joint_vel (14)
- actions (14)
- command (3)
- imitation_phase (2)

**Critic Observations**: 89D
- All actor observations (53D)
- Full reference motion (36D): joint_pos (14) + joint_vel (14) + foot_contacts (2) + base_linear_vel (3) + base_angular_vel (3)

During inference/play, only the 53D actor observations are used.

### Motion Matching

During execution:
1. **Per-environment motion selection**: Each environment independently selects the reference motion closest to its commanded velocity
2. **Phase tracking**: Each environment maintains its own phase, which updates each timestep based on the motion's period
3. **Phase reset**: When an environment resets (due to falling/timeout) or switches to a different reference motion, its phase resets to 0.0
4. **Polynomial evaluation**: The polynomial is evaluated at the current phase to get the reference state
5. **Reward computation**: The reward compares the robot's actual state to the reference state

This ensures that each of the 2048 parallel environments can track different reference motions simultaneously based on their individual commanded velocities.

## Files

- `src/mjlab_microduck/reference_motion.py`: Reference motion loader
- `src/mjlab_microduck/tasks/mdp.py`: Imitation reward and phase observation functions
- `src/mjlab_microduck/tasks/microduck_velocity_env_cfg.py`: Environment configuration
- `src/mjlab_microduck/data/`: Directory for storing reference motion files

**Note**: Use `~/MISC/Open_Duck_reference_motion_generator/scripts/fit_poly.py` to generate polynomial coefficients from reference motions.

## Tuning

You can adjust the reward weights in the environment configuration:

```python
cfg.rewards["imitation"].params["weight_joint_pos"] = 15.0
cfg.rewards["imitation"].params["weight_joint_vel"] = 1e-3
cfg.rewards["imitation"].params["weight_lin_vel_xy"] = 1.0
cfg.rewards["imitation"].params["weight_lin_vel_z"] = 1.0
cfg.rewards["imitation"].params["weight_ang_vel_xy"] = 0.5
cfg.rewards["imitation"].params["weight_ang_vel_z"] = 0.5
cfg.rewards["imitation"].params["weight_contact"] = 1.0
```

You can also adjust the overall imitation reward weight:

```python
cfg.rewards["imitation"].weight = 1.0  # Adjust this value
```

## Verifying Reference Motion Playback

To verify that reference motions are played back correctly, use the replay script:

```bash
# List available motions
python replay_reference_motion.py ./src/mjlab_microduck/data/reference_motions.pkl --list-motions

# Replay a motion with robot hanging in air (to see leg motion clearly)
python replay_reference_motion.py ./src/mjlab_microduck/data/reference_motions.pkl \
    --motion-key "0.01_0.0_-0.4" \
    --hang

# Replay on ground with gravity
python replay_reference_motion.py ./src/mjlab_microduck/data/reference_motions.pkl \
    --motion-key "0.01_0.0_-0.4"

# Replay at different speed
python replay_reference_motion.py ./src/mjlab_microduck/data/reference_motions.pkl \
    --motion-key "0.01_0.0_-0.4" \
    --hang \
    --speed 0.5
```

The script:
- Plays reference motion at 50 Hz (matching training frequency)
- Shows phase progression and foot contacts
- Allows verifying that polynomials are evaluated correctly
- `--hang` mode freezes the robot base in space (pure kinematic replay, no dynamics)
  - Base position stays fixed
  - Only leg joints move according to reference motion
  - Perfect for verifying leg motion patterns

**Interactive keyboard controls:**
- `↑` (Up Arrow) - Switch to maximum forward motion
- `↓` (Down Arrow) - Switch to maximum backward motion
- `←` (Left Arrow) - Switch to maximum left translation
- `→` (Right Arrow) - Switch to maximum right translation
- `A` - Switch to maximum rotate left (counterclockwise)
- `E` - Switch to maximum rotate right (clockwise)
- `S` - Switch to standing motion (zero velocity)
- `Space` - Pause/resume playback

The script automatically finds and categorizes motions by velocity, making it easy to interactively explore different reference motions. Phase resets to 0 when switching motions.

## Inference with Imitation

When running inference with a policy trained using imitation learning, you need to enable imitation mode to include the phase observation:

```bash
python infer_policy.py output.onnx \
    --imitation \
    --reference-motion ./src/mjlab_microduck/data/reference_motions.pkl \
    --lin-vel-x 0.1 \
    --ang-vel-z 0.0
```

The `--imitation` flag:
- Enables phase tracking (updates each timestep based on gait period)
- Adds the 2D phase observation `[cos(phase * 2π), sin(phase * 2π)]` to the policy input
- Changes observation size from 51D to 53D

The `--reference-motion` argument is optional - if not provided, a default gait period of 0.72s is used. Providing the reference motion file ensures the phase tracking uses the correct period from your training data.

**Debug mode:**
```bash
python infer_policy.py output.onnx --imitation --debug
```

This will print the phase value and phase observation at each timestep.
